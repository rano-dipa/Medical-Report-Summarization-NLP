# Medical-Report-Summarization-NLP
Natural Language Processing (NLP) has witnessed signifi
cant advancements, particularly with Large Language Models (LLMs)
like GPT series, revolutionizing text comprehension and generation. In
healthcare, where managing vast amounts of medical data is crucial,
LLMs offer promising solutions for summarizing complex medical re
ports efficiently. This paper explores the effectiveness of LLMs in au
tomating medical report summarization, focusing on both extractive and
abstractive approaches. Through fine-tuning models on the Cochrane
dataset and leveraging state-of-the-art LLMs such as BART, Pegasus,
and T5, we conducted a comparative analysis of summarization quality.
Our methodology involved ensemble modeling and evaluating summaries
using ROUGE metrics, augmented by TF-IDF and Paraphrase Mining
scores for summary selection. Results indicate FLAN-T5 base consis
tently provided optimal summaries. However, resource constraints lim
ited our study, highlighting the need for further exploration with larger
models and datasets. Future research should address these limitations to
advance medical report summarization, ultimately improving healthcare
accessibility and outcomes.
